{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3d3e44",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1582d580",
   "metadata": {},
   "source": [
    "\n",
    "What is a Named Entity?\n",
    "\n",
    "Any word which reprsents a person, organization, location etc. is a Named Entity. Named entity recognition is a subtask of Information Extraction and is the process of identifying words which are named entities in a given text. It is also called entity identification or entity chunking\n",
    "\n",
    "Example\n",
    "\"Apple acquired Zoom in China on Wednesday 6th May 2020\"\n",
    "\n",
    "Here named entities are Apple, Zoom, China and Wednesday 6th May 2020\"\n",
    "Named entity recognition is the task of identifying these words from the text\n",
    "\n",
    "Why it is important?\n",
    "\n",
    "In order to understand the meaning from a given text (for ex a tweet or document), it is important to identify who did what to whom. Named entity recognition is the first task of identifying the words which may represnt the who, what and whom in the text. It helps in identifying the major entities the text is talking about\n",
    "\n",
    "Any NLP task which involves automatically understanding text and acts based on it, needs Named Entity Recognition in its pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb560fce",
   "metadata": {},
   "source": [
    "# Three approaches\n",
    "\n",
    "\n",
    "1) Basic NLTK algorithm\n",
    "\n",
    ".with word segmentation\n",
    "\n",
    ".with sentence segmentation\n",
    "\n",
    "\n",
    "\n",
    "2) Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70f06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb444b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple acquired Zoom in China on Wednesday 6th May 2020.\\\n",
    "This news has made Apple and Google stock jump by 5% on Dow Jones Index in the \\\n",
    "United States of America\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313be92",
   "metadata": {},
   "source": [
    "# Basic Named Entity (NE) tagging using NLTK - Word based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478512c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'acquired',\n",
       " 'Zoom',\n",
       " 'in',\n",
       " 'China',\n",
       " 'on',\n",
       " 'Wednesday',\n",
       " '6th',\n",
       " 'May',\n",
       " '2020.This',\n",
       " 'news',\n",
       " 'has',\n",
       " 'made',\n",
       " 'Apple',\n",
       " 'and',\n",
       " 'Google',\n",
       " 'stock',\n",
       " 'jump',\n",
       " 'by',\n",
       " '5',\n",
       " '%',\n",
       " 'on',\n",
       " 'Dow',\n",
       " 'Jones',\n",
       " 'Index',\n",
       " 'in',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'of',\n",
       " 'America']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize to words\n",
    "words = nltk.word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef02eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91799\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d632fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'NNP'),\n",
       " ('acquired', 'VBD'),\n",
       " ('Zoom', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('China', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('6th', 'CD'),\n",
       " ('May', 'NNP'),\n",
       " ('2020.This', 'CD'),\n",
       " ('news', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('made', 'VBN'),\n",
       " ('Apple', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Google', 'NNP'),\n",
       " ('stock', 'NN'),\n",
       " ('jump', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('5', 'CD'),\n",
       " ('%', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Dow', 'NNP'),\n",
       " ('Jones', 'NNP'),\n",
       " ('Index', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('of', 'IN'),\n",
       " ('America', 'NNP')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of speech tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c76033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\91799\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c90edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#check nltk help for description of the tag\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18635f6",
   "metadata": {},
   "source": [
    "# ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef4460",
   "metadata": {},
   "source": [
    "This task is called \"chunk parsing\" or \"chunking\", and the identified groups are called \"chunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef591577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\91799\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9aae146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\91799\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de2dc246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Apple/NNP)\n",
      "('acquired', 'VBD')\n",
      "('Zoom', 'NNP')\n",
      "('in', 'IN')\n",
      "(NE China/NNP)\n",
      "('on', 'IN')\n",
      "('Wednesday', 'NNP')\n",
      "('6th', 'CD')\n",
      "('May', 'NNP')\n",
      "('2020.This', 'CD')\n",
      "('news', 'NN')\n",
      "('has', 'VBZ')\n",
      "('made', 'VBN')\n",
      "(NE Apple/NNP)\n",
      "('and', 'CC')\n",
      "(NE Google/NNP)\n",
      "('stock', 'NN')\n",
      "('jump', 'NN')\n",
      "('by', 'IN')\n",
      "('5', 'CD')\n",
      "('%', 'NN')\n",
      "('on', 'IN')\n",
      "('Dow', 'NNP')\n",
      "('Jones', 'NNP')\n",
      "('Index', 'NNP')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "(NE United/NNP States/NNPS)\n",
      "('of', 'IN')\n",
      "(NE America/NNP)\n"
     ]
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(pos_tags, binary=True) #either NE or not NE\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7692faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>America</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entities Labels\n",
       "0        America     NE\n",
       "1         Google     NE\n",
       "2          China     NE\n",
       "3  United States     NE\n",
       "4          Apple     NE"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#only taking the Nes\n",
    "\n",
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        #print(chunk)\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "here united states of america is not properly seen , so we are going to arrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000d4cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Apple/NNP)\n",
      "('acquired', 'VBD')\n",
      "(PERSON Zoom/NNP)\n",
      "('in', 'IN')\n",
      "(GPE China/NNP)\n",
      "('on', 'IN')\n",
      "('Wednesday', 'NNP')\n",
      "('6th', 'CD')\n",
      "('May', 'NNP')\n",
      "('2020.This', 'CD')\n",
      "('news', 'NN')\n",
      "('has', 'VBZ')\n",
      "('made', 'VBN')\n",
      "(PERSON Apple/NNP)\n",
      "('and', 'CC')\n",
      "(ORGANIZATION Google/NNP)\n",
      "('stock', 'NN')\n",
      "('jump', 'NN')\n",
      "('by', 'IN')\n",
      "('5', 'CD')\n",
      "('%', 'NN')\n",
      "('on', 'IN')\n",
      "(PERSON Dow/NNP Jones/NNP Index/NNP)\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "(GPE United/NNP States/NNPS)\n",
      "('of', 'IN')\n",
      "(GPE America/NNP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>America</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dow Jones Index</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entities        Labels\n",
       "0            China           GPE\n",
       "1    United States           GPE\n",
       "2          America           GPE\n",
       "3           Google  ORGANIZATION\n",
       "4            Apple        PERSON\n",
       "5             Zoom        PERSON\n",
       "6  Dow Jones Index        PERSON"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#by putting Binary = False\n",
    "\n",
    "chunks = nltk.ne_chunk(pos_tags, binary=False) #either NE or not NE\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    \n",
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        #print(chunk)\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed8016",
   "metadata": {},
   "source": [
    "# Basic Named Entity (NE) tagging using NLTK - Sentence based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a278fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>America</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dow Jones Index</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entities        Labels\n",
       "0            China           GPE\n",
       "1    United States           GPE\n",
       "2          America           GPE\n",
       "3           Google  ORGANIZATION\n",
       "4            Apple        PERSON\n",
       "5             Zoom        PERSON\n",
       "6  Dow Jones Index        PERSON"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "labels = []\n",
    "\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "for sent in sentence:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)),binary=False):\n",
    "        if hasattr(chunk,'label'):\n",
    "            entities.append(' '.join(c[0] for c in chunk))\n",
    "            labels.append(chunk.label())\n",
    "            \n",
    "entities_labels = list(set(zip(entities,labels)))\n",
    "\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654aa81b",
   "metadata": {},
   "source": [
    "# Using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a272069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy import displacy\n",
    "#SpaCy 2.x brough significant speed and accuracy improvements\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ef1f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (61.2.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91799\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.22.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\91799\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91799\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91799\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.3.0\n",
      "    Uninstalling en-core-web-sm-3.3.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.3.0\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 22:53:41.738464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2024-05-07 22:53:41.738511: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-07 22:53:47.604357: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2024-05-07 22:53:47.604388: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-05-07 22:53:47.607298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-DQM1PJAK\n",
      "2024-05-07 22:53:47.607392: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-DQM1PJAK\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "831fc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a948f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Position_Start</th>\n",
       "      <th>Position_End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Apple)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Zoom)</td>\n",
       "      <td>GPE</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(China)</td>\n",
       "      <td>GPE</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Wednesday, 6th, May)</td>\n",
       "      <td>DATE</td>\n",
       "      <td>32</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Apple)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>74</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Google)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>84</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(5, %)</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>105</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Dow, Jones, Index)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>111</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(the, United, States, of, America)</td>\n",
       "      <td>GPE</td>\n",
       "      <td>130</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Entities   Labels  Position_Start  Position_End\n",
       "0                             (Apple)      ORG               0             5\n",
       "1                              (Zoom)      GPE              15            19\n",
       "2                             (China)      GPE              23            28\n",
       "3               (Wednesday, 6th, May)     DATE              32            49\n",
       "4                             (Apple)      ORG              74            79\n",
       "5                            (Google)      ORG              84            90\n",
       "6                              (5, %)  PERCENT             105           107\n",
       "7                 (Dow, Jones, Index)      ORG             111           126\n",
       "8  (the, United, States, of, America)      GPE             130           158"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent)\n",
    "    labels.append(ent.label_)\n",
    "    position_start.append(ent.start_char)\n",
    "    position_end.append(ent.end_char)\n",
    "    \n",
    "df = pd.DataFrame({'Entities':entities,'Labels':labels,'Position_Start':position_start, 'Position_End':position_end})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3190f28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8d4e3",
   "metadata": {},
   "source": [
    "Spacy works the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8551d",
   "metadata": {},
   "source": [
    "What you can build with this?\n",
    "\n",
    "A bot that can analyze financial news and extract information about entities that are mentioned in a\n",
    "given article along with location, dates and numeric information. This information can be further utilized\n",
    "in building algorithmic trading bots\n",
    "\n",
    "Analyze research papers produced everyday on COVID19 and find out any significant developments\n",
    "1\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e5c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
